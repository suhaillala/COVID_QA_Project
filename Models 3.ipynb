{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast, GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import time, math, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from tokenizers import CharBPETokenizer\n",
    "import tokenizers\n",
    "import operator\n",
    "from queue import PriorityQueue\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Import from external file\n",
    "X_train = pd.read_csv(\"output/x_train.csv\")\n",
    "X_test = pd.read_csv(\"output/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 5000\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Set paths\n",
    "train_path = 'output/x_train_nn.csv'\n",
    "test_path = 'output/x_test_nn.csv'\n",
    "\n",
    "# Save files\n",
    "X_train[['question', 'prep_answer', 'cluster']].to_csv(train_path, index=False, header=False)\n",
    "X_test[['question', 'prep_answer', 'cluster']].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "# Create pytorch variables\n",
    "src = torchtext.data.Field(\n",
    "    include_lengths=True,\n",
    "    lower=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN],\n",
    "    lower=True,\n",
    "    is_target=True\n",
    "    )\n",
    "cluster = torchtext.data.Field(\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt), ('cluster', cluster)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "\n",
    "data_test = torchtext.data.TabularDataset(\n",
    "        path=test_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt), ('cluster', cluster)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'the', 'to', 'of', 'a', 'i', 'in', 'and', 'is', 'that', 'for', 'this', 'be', 'it', 'are', 'have', 'my', 'on', 'with']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', 'the', 'to', 'of', 'and', 'a', 'is', 'in', 'that', 'for', 'you', 'not', 'are', 'be', 'it', 'or', 'with', 'have', 'as']\n",
      "\n",
      "num training examples: 1034\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['we', 'often', 'get', 'sore', 'throats', 'once', 'or', 'twice', 'a', 'year,', 'and', 'it', 'clears', 'in', 'a', 'few', 'days', 'sometimes', 'without', 'any', 'antibiotics.', 'i', 'was', 'wondering', 'why', \"doesn't\", 'our', 'body', 'become', 'immune', 'after', 'clearing', 'a', 'sore', 'throat?']\n",
      "tgt:\n",
      " ['<sos>', 'short', 'answer', 'it', 'should', 'be', 'noted', 'that', 'there', 'are', 'many', 'non-pathogenic', 'causes', 'of', 'sore', 'throat,', 'and', 'i', 'would', 'suspect', 'that', 'you', 'are', 'not', 'always', 'distinguishing', 'these', 'causes', 'from', 'actual', 'illness.', 'in', 'most', 'cases,', 'it', 'would', 'not', 'be', 'possible', 'to', 'build', 'a', 'response', 'immunological', 'or', 'otherwise', 'to', 'avoid', 'reacting', 'to', 'these', 'stressors.', 'bacteria', 'and', 'viruses', 'mutate', 'frequently', 'in', 'ways', 'that', 'make', 'your', 'immune', 'system', 'unable', 'to', 'properly', 'recognize', 'them', 'to', 'ward', 'off', 'initial', 'infection.', 'your', 'immune', 'system', 'recognizes', 'specific', 'pathogens', 'based', 'on', 'structural', 'characteristics.', 'if', 'those', 'characteristics', 'change', 'when', 'pathogens', 'mutate', 'which', 'happens', 'fairly', 'frequently', ',', 'your', 'immune', 'system', 'will', 'not', 'recognize', 'it', 'as', 'readily', 'or', 'initially', 'at', 'all', '.', 'long', 'answer', 'sore', 'throat', 'is', 'most', 'commonly', 'attributed', 'to', 'pharyngitis,', 'or', 'the', 'inflammation', 'of', 'the', 'back', 'of', 'the', 'throat', 'called', 'the', 'pharynx.', 'causes', 'sore', 'throats', 'can', 'be', 'caused', 'by', 'many', 'things.', 'e.g.,', 'see', 'here,', 'summarized', 'below', ':', 'first', 'off,', 'a', 'sore', 'throat', 'can', 'be', 'the', 'result', 'of', 'many', 'things', 'unrelated', 'to', 'illness:', 'allergies', 'dryness', 'irritants', 'excessive', 'talking,singing,shouting,etc.', 'muscle', 'strain', 'gastroesophageal', 'reflux', 'disease', 'gerd', 'as', 'a', 'result,', 'at', 'least', '30', 'higher', 'in', 'adults', 'of', 'reported', 'sore', 'throat', 'cases', 'and', 'supposedly', 'many', 'more', 'unreported', 'cases', 'are', 'not', 'due', 'to', 'any', 'type', 'of', 'illness.', 'further,', 'a', 'number', 'of', 'pathogens', 'can', 'cause', 'a', 'sore', 'throat:', 'viruses', 'rhinovirus,', 'coronavirus,', 'influenza,', 'parainfluenza,', 'adenoviruses,', 'epstein-barr,', 'etc.', 'see', 'here', 'for', 'one', 'estimate', 'of', 'relative', 'etiologies', '.', 'bacteria', 'a', 'number', 'of', 'bacterial', 'infections', 'can', 'cause', 'a', 'sore', 'throat.', 'the', 'most', 'common', 'is', 'streptococcus', 'pyogenes', 'aka', 'strep', 'a', '.', 'it', 'turns', 'out', 'that', 'pathogen-caused', 'pharyngitis', 'is', 'fairly', 'common.', 'supposedly', '10', 'of', 'adults', 'with', 'sore', 'throats', 'have', 'strep,', 'and', 'around', '70', 'of', 'children', 'pharyngitis', 'cases', 'seen', 'by', 'doctors', 'are', 'either', 'virally', 'or', 'bacterially', 'caused.', 'however,', 'it', 'should', 'be', 'noted', 'that', 'only', '20-30', 'of', 'children', 'and', 'just', '5-10', 'of', 'adults', 'have', 'bacteria', 'to', 'blame', 'for', 'their', 'sore', 'throat,', 'with', 'viral', 'infections', 'being', 'much', 'more', 'common.', 'source:', 'idsa', '.', 'sore', 'throat', 'frequency', 'it', 'turns', 'out', 'that', 'sore', 'throats', 'are', 'fairly', 'common', 'jones', '2004', 'determined', 'that', 'about', '7.5', 'of', 'people', 'have', 'a', 'sore', 'throat', 'in', 'any', 'three-month', 'period.', 'further,', 'on', 'average,', 'adults', 'get', 'a', 'sore', 'throat', 'two', 'to', 'three', 'times', 'a', 'year', 'and', 'children', 'about', 'five', 'times', 'a', 'year', 'tamparo', '2011', 'and', 'rutter', 'amp', 'newby', '2015', ',', 'so', 'your', 'personal', 'experience', 'is', 'fairly', 'average.', 'because', 'viral', 'causes', 'and', 'non-pathogenic', 'causes', 'make', 'up', 'an', 'overwhelming', 'percentage', 'of', 'overall', 'sore', 'throat', 'cases,', 'it', 'should', 'not', 'be', 'surprising', 'that', 'most', 'sore', 'throats', 'clear', 'up', 'without', 'the', 'use', 'of', 'any', 'antibiotics.', 'as', 'for', 'those', 'that', 'are', 'caused', 'by', 'pathogens:', 'why', 'no', 'immunity', 'bacteria', 'and', 'viruses', 'both', 'mutate', 'fairly', 'quickly', 'see', 'antigenic', 'variation,', 'antigenic', 'drift', 'and', 'antigenic', 'shift', ',', 'so', 'it', 'should', 'not', 'be', 'surprising', 'that', 'you', 'are', 'capable', 'of', 'having', 'an', 'immune', 'response', 'to', 'pathogens', 'throughout', 'life.', 'if', 'we', 'consider', 'just', 'strep', 'alone,', 'it', 'mutates/reemerges', 'with', 'improved', 'fitness', 'frequently', 'e.g.,', 'see', 'bao', 'et', 'al', '2016', 'and', 'their', 'citations', 'and', 'see', 'sanjuan', 'et', 'al.', '2010', 'for', 'a', 'reviewof', 'viral', 'mutation', 'rates.', 'so', 'not', 'only', 'are', 'there', 'a', 'number', 'of', 'pathogens', 'that', 'can', 'cause', 'sore', 'throat,', 'but', 'each', 'of', 'these', 'pathogens', 'can', 'mutate.', 'that', 'leaves', 'you', 'with', 'a', 'fairly', 'sizable', 'number', 'of', 'potential', 'invaders', 'that', 'can', 'illicit', 'a', 'sore', 'throat.', 'let', 'is', 'see', 'how', 'your', 'immune', 'system', 'responds:', 'your', 'immune', 'system', 'contains', 'cells', 'that', 'can', 'be', 'split', 'into', 'two', 'broad', 'groups:', 'cells', 'that', 'belong', 'to', 'the', 'adaptive', 'immune', 'system,', 'and', 'cells', 'which', 'belong', 'to', 'the', 'innate', 'immune', 'system.', 'see', 'here', 'for', 'a', 'simple', 'explanation', '.', 'innate', 'immune', 'system:', 'consists', 'of', 'cells', 'responsible', 'for', 'initial', 'detection', 'of', 'pathogens,', 'recruitment', 'of', 'other', 'immune', 'cells,', 'activation', 'of', 'the', 'complement', 'system,', 'engulfment', 'and', 'more', 'immediate', 'destruction', 'of', 'pathogens.', 'adaptive', 'immune', 'system:', 'consists', 'of', 'cells', 'which', 'coordinate', 'to', 'develop', 'a', 'memory', 'of', 'the', 'infections', 'they', 'have', 'seen', 'previously', 'in', 'order', 'to', 'mount', 'a', 'faster,', 'more', 'pronounced', 'immune', 'response', 'to', 'repeat', 'pathogens.', 'depending', 'on', 'the', 'kind', 'of', 'foreign', 'invasion,', 'two', 'different', 'immune', 'responses', 'can', 'occur', 'in', 'either', 'of', 'these', '2', 'broader', 'systems', ':', 'either', 'a', 'humoral', 'response', 'involving', 'antibodies', 'or', 'a', 'cell-mediated', 'repsponse.', 'a', 'general', 'feature', 'of', 'the', 'immune', 'system', 'is', 'that', 'these', 'mechanisms', 'rely', 'on', 'detecting', 'structural', 'features', 'of', 'the', 'pathogen', 'or', 'toxin', 'that', 'mark', 'it', 'as', 'distinct', 'from', 'host', 'cells', 'see', 'chaplin', '2010', 'for', 'a', 'review', '.', 'the', 'mechanisms', 'permitting', 'recognition', 'of', 'pathogenic/toxic', 'foreign', 'structures', 'can', 'be', 'broken', 'down', 'into', 'two', 'general', 'categories:', 'hard-wired', 'responses', 'encoded', 'by', 'genes', 'that', 'recognize', 'molecular', 'patterns', 'shared', 'both', 'by', 'many', 'microbes', 'and', 'toxins', 'that', 'are', 'not', 'present', 'in', 'the', 'mammalian', 'host.', 'this', 'is', 'the', 'innate', 'system', 'described', 'above.', 'responses', 'that', 'are', 'encoded', 'by', 'gene', 'elements', 'that', 'somatically', 'rearrange', 'to', 'assemble', 'antigen-binding', 'molecules', 'with', 'extreme', 'specificity', 'for', 'individual', 'unique', 'foreign', 'structures.', 'this', 'is', 'the', 'adaptive', 'system', 'described', 'above.', 'antibodies', 'recognizes', 'a', 'unique/specific', 'molecule', 'of', 'the', 'harmful', 'agent,', 'called', 'an', 'antigen,', 'and', 'binds', 'to', 'it', 'to', 'tag', 'it', 'for', 'immune', 'attack.', 'because', 'the', 'adaptive', 'system', 'is', 'composed', 'of', 'small', 'numbers', 'of', 'cells', 'with', 'specificity', 'for', 'any', 'individual', 'pathogen', 'the', 'responding', 'cells', 'must', 'proliferate', 'after', 'encountering', 'the', 'antigen', 'in', 'order', 'to', 'attain', 'sufficient', 'numbers', 'to', 'mount', 'an', 'effective', 'response', 'against', 'the', 'microbe', 'or', 'the', 'toxin.', 'source:', 'chaplin', '2010', '.', 'this', 'means', 'that', 'you', 'can', 'still', 'experience', 'some', 'mild', 'symptoms', 'due', 'to', 'the', 'invading', 'pathogen.', 'the', 'important', 'take-away', 'from', 'all', 'this,', 'though,', 'is', 'that', 'your', 'immune', 'response', 'relies', 'on', 'structural', 'markers', 'on', 'pathogens', 'in', 'order', 'to', 'recognize', 'them.', 'this', 'is', 'true', 'for', 'both', 'innate', 'and', 'adaptive', 'immune', 'responses.', 'if', 'the', 'physical/structural', 'properties', 'of', 'a', 'pathogen', 'change', 'due', 'to', 'antigenic', 'processes', 'and', 'mutations', 'as', 'mentioned', 'above', 'your', 'immune', 'system', 'will', 'be', 'slower', 'and', 'possibly', 'ineffective', 'in', 'responding', 'to', 'an', 'attack.', 'the', 'result,', 'then,', 'could', 'be', 'another', 'one', 'of', 'those', 'sore', 'throats...', 'citations', 'bao,', 'yun-juan', 'et', 'al.', 'phenotypic', 'differentiation', 'of', 'streptococcus', 'pyogenes', 'populations', 'is', 'induced', 'by', 'recombination-driven', 'gene-specific', 'sweeps.', 'scientific', 'reports', '6', '2016', ':', '36644.', 'pmc.', 'web.', '17', 'mar.', '2017.', 'chaplin,', 'd.', 'd.', '2010', '.', 'overview', 'of', 'the', 'immune', 'response.', 'journal', 'of', 'allergy', 'and', 'clinical', 'immunology,', '125', '2', ',', 's3-s23.', 'jones,', 'roger', '2004', '.', 'oxford', 'textbook', 'of', 'primary', 'medical', 'care.', 'oxford', 'university', 'press.', 'p.', '674.', 'isbn', '9780198567820.', 'retrieved', '4', 'august', '2016.', 'rutter,', 'paul', 'professor', 'newby,', 'david', '2015', '.', 'community', 'pharmacy', 'anz:', 'symptoms,', 'diagnosis', 'and', 'treatment.', 'elsevier', 'health', 'sciences.', 'p.', '19.', 'isbn', '9780729583459.', 'sanjuan,', 'r.,', 'nebot,', 'm.', 'r.,', 'chirico,', 'n.,', 'mansky,', 'l.', 'm.,', 'amp', 'belshaw,', 'r.', '2010', '.', 'viral', 'mutation', 'rates.', 'journal', 'of', 'virology,', '84', '19', ',', '9733-9748.', 'tamparo,', 'carol', '2011', '.', 'fifth', 'edition:', 'diseases', 'of', 'the', 'human', 'body.', 'philadelphia,', 'pa:', 'f.a.', 'davis', 'company.', 'p.', '356.', 'isbn', '978-0-8036-2505-1.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "cluster.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(200, n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network.\n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)\n",
    "\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "\n",
    "        #out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        # if GPU is available\n",
    "        if (torch.cuda.is_available()):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "\n",
    "        # if GPU is not available\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "\n",
    "        return hidden\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, dropout=0.1):\n",
    "        '''\n",
    "        Illustrative decoder\n",
    "        '''\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=output_size,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      )\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, bidirectional=True, dropout=dropout, batch_first=False)\n",
    "        self.dropout_rate = dropout\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).transpose(0, 1)  # [B,1] -> [ 1, B, D]\n",
    "        embedded = F.dropout(embedded, self.dropout_rate)\n",
    "\n",
    "        output = embedded\n",
    "\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        out = self.out(output.squeeze(0))\n",
    "        output = F.log_softmax(out, dim=1)\n",
    "        return output, hidden\n",
    "\n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, cluster, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "\n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.init_hidden(batch_size)\n",
    "\n",
    "    # zero accumulated gradients\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.n_hidden, device=device)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "\n",
    "    # pass the inputs through the encoder\n",
    "    #for ei in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        #encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=device)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "        topv, topi = decoder_output.topk(2)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "\n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "    nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
    "    nn.utils.clip_grad_norm_(decoder.parameters(), 1)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, batch_size=32, print_every=10000, learning_rate=0.04, teacher_forcing_ratio=0.2):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = AdamW(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = AdamW(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # note batch size of 1, just for simplicity\n",
    "    # DO NOT INCREASE THE BATCH SIZE\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=batch_size,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=device, repeat=False)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "\n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "            cluster = getattr(batch, 'cluster')\n",
    "\n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            #input_tensor = input_tensor[0]\n",
    "            #target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, cluster, batch_size, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "\n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "\n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(len(input_vocab), hidden_size).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size, hidden_size, len(output_vocab)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 10 epochs...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (4, 613, 256), got [4, 32, 256]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-39-9861757ba5f7>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainIters\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mencoder1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprint_every\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m10000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-31-7f75730b3a53>\u001B[0m in \u001B[0;36mtrainIters\u001B[1;34m(encoder, decoder, n_iters, batch_size, print_every, learning_rate, teacher_forcing_ratio)\u001B[0m\n\u001B[0;32m     35\u001B[0m             \u001B[1;31m#target_tensor = target_tensor[0]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 37\u001B[1;33m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_tensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_tensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcluster\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_optim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder_optim\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mteacher_forcing_ratio\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mteacher_forcing_ratio\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     38\u001B[0m             \u001B[0mprint_loss_total\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m             \u001B[0mprint_loss_epoch\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-38-9c1adda2492e>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(input_tensor, target_tensor, cluster, batch_size, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length, teacher_forcing_ratio)\u001B[0m\n\u001B[0;32m     23\u001B[0m     \u001B[1;31m# pass the inputs through the encoder\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[1;31m#for ei in range(input_length):\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 25\u001B[1;33m     \u001B[0mencoder_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_hidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_tensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_hidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     26\u001B[0m         \u001B[1;31m#encoder_outputs[ei] = encoder_output[0, 0]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-29-8dfb99963a0e>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x, hidden)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[1;31m## Get the outputs and the new hidden state from the lstm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 31\u001B[1;33m         \u001B[0mlstm_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedded\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     32\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m         \u001B[1;31m## pass through a dropout layer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    725\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    726\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 727\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    728\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    729\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    577\u001B[0m             \u001B[0mhx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpermute_hidden\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msorted_indices\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 579\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheck_forward_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    580\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    581\u001B[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001B[1;32md:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mcheck_forward_args\u001B[1;34m(self, input, hidden, batch_sizes)\u001B[0m\n\u001B[0;32m    531\u001B[0m         \u001B[0mexpected_hidden_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_expected_hidden_size\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 533\u001B[1;33m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001B[0m\u001B[0;32m    534\u001B[0m                                'Expected hidden[0] size {}, got {}')\n\u001B[0;32m    535\u001B[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n",
      "\u001B[1;32md:\\programdata\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mcheck_hidden_size\u001B[1;34m(self, hx, expected_hidden_size, msg)\u001B[0m\n\u001B[0;32m    194\u001B[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001B[0;32m    195\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mhx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0mexpected_hidden_size\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 196\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexpected_hidden_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    197\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    198\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcheck_forward_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected hidden[0] size (4, 613, 256), got [4, 32, 256]"
     ]
    }
   ],
   "source": [
    "trainIters(encoder1, decoder1, 10, print_every=10000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}