{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, BertTokenizerFast, GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import time, math, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from tokenizers import CharBPETokenizer\n",
    "import tokenizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Import from external file\n",
    "X_train = pd.read_csv(\"output/x_train.csv\")\n",
    "X_test = pd.read_csv(\"output/x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "MAX_LEN = 5000\n",
    "\n",
    "def len_filter(example):\n",
    "    return len(example.src) <= MAX_LEN and len(example.tgt) <= MAX_LEN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set paths\n",
    "train_path = 'output/x_train_nn.csv'\n",
    "test_path = 'output/x_test_nn.csv'\n",
    "\n",
    "# Save files\n",
    "X_train[['question', 'prep_answer', 'cluster']].to_csv(train_path, index=False, header=False)\n",
    "X_test[['question', 'prep_answer', 'cluster']].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "# Create pytorch variables\n",
    "src = torchtext.data.Field(\n",
    "    include_lengths=True,\n",
    "    lower=True\n",
    "    )\n",
    "tgt = torchtext.data.Field(\n",
    "    preprocessing = lambda seq: [SOS_TOKEN] + seq + [EOS_TOKEN],\n",
    "    lower=True\n",
    "    )\n",
    "cluster = torchtext.data.Field(\n",
    "    )\n",
    "\n",
    "data_train = torchtext.data.TabularDataset(\n",
    "        path=train_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt), ('cluster', cluster)],\n",
    "        filter_pred=len_filter\n",
    "    )\n",
    "\n",
    "data_test = torchtext.data.TabularDataset(\n",
    "        path=test_path, format='csv',\n",
    "        fields=[('src', src), ('tgt', tgt), ('cluster', cluster)],\n",
    "        filter_pred=len_filter\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens from input vocab:\n",
      " ['<unk>', '<pad>', 'the', 'to', 'of', 'a', 'i', 'in', 'and', 'is', 'that', 'for', 'this', 'be', 'it', 'are', 'have', 'my', 'on', 'with']\n",
      "\n",
      "20 tokens from output vocab:\n",
      " ['<unk>', '<pad>', 'the', 'to', 'of', 'and', 'a', 'in', 'is', 'that', 'for', 'you', 'are', 'be', 'with', 'or', 'it', 'have', 'not', 'as']\n",
      "\n",
      "num training examples: 1034\n",
      "\n",
      "example train data:\n",
      "src:\n",
      " ['can', 'coronavirus', 'go', 'through', 'skin', 'and', 'into', 'the', 'body?']\n",
      "tgt:\n",
      " ['<sos>', '“it', 'may', 'be', 'possible', 'that', 'a', 'person', 'can', 'get', 'covid-19', 'by', 'touching', 'a', 'surface', 'or', 'object', 'that', 'has', 'the', 'virus', 'on', 'it', 'and', 'then', 'touching', 'their', 'own', 'mouth,', 'nose,', 'or', 'possibly', 'their', 'eyes,', 'but', 'this', 'is', 'not', 'thought', 'to', 'be', 'the', 'main', 'way', 'the', 'virus', 'spreads,”', 'the', 'cdc', 'says.', 'more', 'often', 'than', 'not,', 'people', 'get', 'coronavirus', 'through', 'respiratory', 'droplets', 'produced', 'when', 'an', 'infected', 'person', 'coughs', 'or', 'sneezes.', '“these', 'droplets', 'can', 'land', 'in', 'the', 'mouths', 'or', 'noses', 'of', 'people', 'who', 'are', 'nearby', 'or', 'possibly', 'be', 'inhaled', 'into', 'the', 'lungs,”', 'the', 'cdc', 'says.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "src.build_vocab(data_train, max_size=50000)\n",
    "tgt.build_vocab(data_train, max_size=50000)\n",
    "input_vocab = src.vocab\n",
    "output_vocab = tgt.vocab\n",
    "\n",
    "print('20 tokens from input vocab:\\n', list(input_vocab.stoi.keys())[:20])\n",
    "print('\\n20 tokens from output vocab:\\n', list(output_vocab.stoi.keys())[:20])\n",
    "\n",
    "print('\\nnum training examples:', len(data_train.examples))\n",
    "\n",
    "item = random.choice(data_train.examples)\n",
    "print('\\nexample train data:')\n",
    "print('src:\\n', item.src)\n",
    "print('tgt:\\n', item.tgt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_qa(questions, answers, tokenizer, maxlen=1024):\n",
    "    \"\"\"\n",
    "    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer.enable_truncation(max_length=maxlen)\n",
    "    tokenizer.enable_padding(pad_id=1, pad_token = \"<pad>\")\n",
    "    all_ids = []\n",
    "\n",
    "    for i in range(0, len(questions)):\n",
    "        q = questions[i]\n",
    "        a = answers[i]\n",
    "\n",
    "        encs = tokenizer.encode(q, a)\n",
    "        all_ids.append(encs.ids)\n",
    "        if len(encs.ids) > 512:\n",
    "            return q, a\n",
    "\n",
    "    return np.array(all_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = CharBPETokenizer(bert_normalizer=False)\n",
    "\n",
    "#generate word vocab from the text\n",
    "tokenizer.train([\"data/elon.txt\"], special_tokens=[\n",
    "          \"<s>\",\n",
    "          \"<pad>\",\n",
    "          \"</s>\" ,\n",
    "          \"<mask>\"\n",
    "])\n",
    "\n",
    "#Preprocess the token so that each\n",
    "#start with start of sequence <s> and end with end of sequence </s>\n",
    "tokenizer._tokenizer.post_processor = tokenizers.processors.BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "#enable padding\n",
    "tokenizer.enable_padding(pad_id=1, pad_token = \"<pad>\")\n",
    "\n",
    "#truncate to maximum length\n",
    "tokenizer.enable_truncation(max_length=1024)\n",
    "\n",
    "#create a token data\n",
    "token_data = [x.ids for x in tokenizer.encode_batch(data_clean)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import math\n",
    "def attention(query, key, value, mask=None, dropout=0.0):\n",
    "\n",
    "  d_k = query.size(-1)\n",
    "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "  if mask is not None:\n",
    "    scores = scores.masked_fill(mask ==0, -1e-9)\n",
    "  p_attn = F.softmax(scores, dim= -1)\n",
    "  p_attn = F.dropout(p_attn, p=dropout)\n",
    "  return torch.matmul(p_attn, value), p_attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import copy\n",
    "def clones(module, N):\n",
    "    \"\"\"Produce N identical layers.\"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, h, d_model, dropout=0.1):\n",
    "    super(MultiHeadedAttention, self).__init__()\n",
    "    assert d_model % h == 0\n",
    "\n",
    "    self.d_k = d_model // h\n",
    "    self.h = h\n",
    "    self.p = dropout\n",
    "    self.linears = clones(nn.Linear(d_model,d_model),4)\n",
    "    self.attn = None\n",
    "\n",
    "  def forward(self, query, key, value, mask=None):\n",
    "\n",
    "    if mask is not None:\n",
    "      mask = mask.unsqueeze(1)\n",
    "\n",
    "    nbatches = query.size(0)\n",
    "\n",
    "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
    "                        for l,x in zip(self.linears, (query, key, value))]\n",
    "    x, self_attn = attention(query, key, value, mask=mask, dropout=self.p)\n",
    "\n",
    "    x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "\n",
    "    return self.linears[-1](x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Implements FFN equation.\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Implement the PE function.\"\"\"\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, :x.size(1)],\n",
    "                         requires_grad=False)\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\n",
    "  def __init__(self, decoder, tgt_embed, generator):\n",
    "    super(GPT, self).__init__()\n",
    "    self.decoder = decoder\n",
    "    self.embed = tgt_embed\n",
    "    self.generator = generator\n",
    "\n",
    "  def forward(self, x, x_mask):\n",
    "\n",
    "    x = self.embed(x)\n",
    "    x =  self.decoder(x,x_mask)\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Construct a layernorm module (See citation for details).\"\"\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "  def __init__(self, layer, N):\n",
    "\n",
    "    super(Decoder, self).__init__()\n",
    "    self.layers = clones(layer, N)\n",
    "    self.norm = LayerNorm(layer.size)\n",
    "\n",
    "  def forward(self,x, tgt_mask):\n",
    "\n",
    "    for layer in self.layers:\n",
    "      x = layer(x, tgt_mask)\n",
    "    return self.norm(x)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "  def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    self.size = size\n",
    "    self.self_attn = self_attn\n",
    "    self.feed_forward = feed_forward\n",
    "    self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "\n",
    "  def forward(self, x, tgt_mask):\n",
    "\n",
    "    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "    return self.sublayer[1](x, self.feed_forward)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def make_model(eng_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "\n",
    "  c = copy.deepcopy\n",
    "  attn = MultiHeadedAttention(h,d_model)\n",
    "  ff   = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "  position = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "\n",
    "  model = GPT(\n",
    "      Decoder(DecoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "      nn.Sequential(Embeddings(d_model, eng_vocab), c(position)),\n",
    "      Generator(d_model, eng_vocab)\n",
    "  )\n",
    "\n",
    "  for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "      nn.init.xavier_uniform(p)\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, myinput, hidden):\n",
    "        embedded = self.embedding(myinput).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,\n",
    "          max_length=MAX_LEN, teacher_forcing_ratio=0.5):\n",
    "\n",
    "    # get an initial hidden state for the encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    # zero the gradients of the optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # get the seq lengths, used for iterating through encoder/decoder\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    # create empty tensor to fill with encoder outputs\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    # create a variable for loss\n",
    "    loss = 0\n",
    "\n",
    "    # pass the inputs through the encoder\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    # create a start-of-sequence tensor for the decoder\n",
    "    decoder_input = torch.tensor([[output_vocab.stoi[SOS_TOKEN]]], device=device)\n",
    "\n",
    "    # set the decoder hidden state to the final encoder hidden state\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # decide if we will use teacher forcing\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "        loss += criterion(decoder_output, target_tensor[di].unsqueeze(0))\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "\n",
    "        if decoder_input.item() == output_vocab.stoi[EOS_TOKEN]:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=10000, learning_rate=0.04, teacher_forcing_ratio=0.2):\n",
    "    print(f'Running {n_iters} epochs...')\n",
    "    print_loss_total = 0\n",
    "    print_loss_epoch = 0\n",
    "\n",
    "    encoder_optim = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optim = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # note batch size of 1, just for simplicity\n",
    "    # DO NOT INCREASE THE BATCH SIZE\n",
    "    batch_iterator = torchtext.data.Iterator(\n",
    "        dataset=data_train, batch_size=1,\n",
    "        sort=False, sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.src),\n",
    "        device=device, repeat=False)\n",
    "\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(n_iters):\n",
    "        batch_generator = batch_iterator.__iter__()\n",
    "        step = 0\n",
    "        start = time.time()\n",
    "        for batch in batch_generator:\n",
    "            step += 1\n",
    "\n",
    "            # get the input and target from the batch iterator\n",
    "            input_tensor, input_lengths = getattr(batch, 'src')\n",
    "            target_tensor = getattr(batch, 'tgt')\n",
    "\n",
    "            # this is because we're not actually using the batches.\n",
    "            # batch size is 1 and this just selects that first one\n",
    "            input_tensor = input_tensor[0]\n",
    "            target_tensor = target_tensor[0]\n",
    "\n",
    "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, criterion, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            print_loss_total += loss\n",
    "            print_loss_epoch += loss\n",
    "\n",
    "\n",
    "            if step % print_every == 0:\n",
    "                print_loss_avg = print_loss_total / print_every\n",
    "                print_loss_total = 0\n",
    "                t = (time.time() - start) / 60\n",
    "                print(f'step: {step}\\t avg loss: {print_loss_avg:.2f}\\t time for {print_every} steps: {t:.2f} min')\n",
    "                start = time.time()\n",
    "\n",
    "        print_loss_avg = print_loss_epoch / step\n",
    "        print_loss_epoch = 0\n",
    "        print(f'End of epoch {e}, avg loss {print_loss_avg:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}